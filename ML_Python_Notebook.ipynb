{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a2362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625f4bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eda58605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2e1200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('/Users/wushuangyan/Desktop/STAT classes/Linear models/base'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "789af8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/wushuangyan/Desktop/STAT classes/Linear models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53134fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipynb.fs.full.module import GradientClassifier #import from Jupter online files\n",
    "from base.module import GradientClassifier  # import from local files\n",
    "\n",
    "class LogisticRegression(GradientClassifier):\n",
    "    def __init__(self, feat_dim=4):\n",
    "        GradientClassifier.__init__(self, \"LogReg\")\n",
    "        self._features = feat_dim\n",
    "        self.weights = tc.nn.Parameter(tc.randn((feat_dim, 1), dtype=tc.float32))\n",
    "        self.bias = tc.nn.Parameter(tc.zeros((1, ), dtype=tc.float32) + 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 2 and x.shape[1] == self._features\n",
    "        logits = tc.mm(x, self.weights) + self.bias   # \"\"\"Problem 1. Implement a linear model.\"\"\"\n",
    "        return tc.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a3dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipynb.fs.full.module import GradientClassifier\n",
    "from base.module import GradientClassifier\n",
    "\n",
    "class MLPBinaryClassifier(GradientClassifier):\n",
    "    def __init__(self, in_dim=4, hide_dim=64, device=\"cpu\"):\n",
    "        GradientClassifier.__init__(self, \"MLP\")\n",
    "        self._features = in_dim\n",
    "        self.weights_hide = tc.nn.Parameter(tc.randn((in_dim, hide_dim)))\n",
    "        self.bias_hide = tc.nn.Parameter(tc.zeros((hide_dim, ), dtype=tc.float32) + 0.1)\n",
    "        self.weights_clf = tc.nn.Parameter(tc.randn((hide_dim, 1)))\n",
    "        self.bias_clf = tc.nn.Parameter(tc.zeros((1, ), dtype=tc.float32) + 0.1)\n",
    "\n",
    "    def forward(self, x): \n",
    "        assert len(x.shape) == 2 and x.shape[1] == self._features\n",
    "                              #\"\"\"Problem 1. Implement the MLP model.\"\"\"\n",
    "        logits = tc.mm((tc.mm (x, self.weights_hide) + self.bias_hide), self.weights_clf)+ self.bias_clf\n",
    "        return tc.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef318996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ipynb.fs.full.module import StatisticClassifier\n",
    "from base.module import StatisticClassifier\n",
    "\n",
    "class BayesianClassifier(StatisticClassifier):\n",
    "    \n",
    "    def __init__(self):\n",
    "        StatisticClassifier.__init__(self, \"NaiveBayesian\")\n",
    "        self._probas = None\n",
    "\n",
    "    def forward(self, sample):\n",
    "        target_y, target_prob = 0, 0.0\n",
    "        for y, (postior, cond_probs) in enumerate(self._probas):\n",
    "            # P(Y=y|x1, x2, ...) ~= P(Y=y) * P(X1=x1|Y=y) * P(X2=x2|Y=y) * ...\n",
    "            for x_idx, x_cond_prob_dict in enumerate(cond_probs):\n",
    "                postior *= x_cond_prob_dict.get(sample[x_idx], 0.0)\n",
    "            if  postior > target_prob:       # Problem 1. find the biggest postior probability\n",
    "                target_y, target_prob = y, postior           \n",
    "        return target_y            \n",
    "\n",
    "    def _fit(self, X, Y):\n",
    "        # the dimsension of is X = (num_samples, num_featues)\n",
    "        self._probas = [] # store P(y) and P(X|Y=y) as a tuple\n",
    "        for y in range(self._num_cls): # y= 0,1,2,3...\n",
    "                                           # calculate P(X|Y=y) by storing them as a sequence\n",
    "            subX = X[Y == y] # records with label y\n",
    "            y_cond_prob = [] # store [P(X1|Y=y), P(X2|Y=y), ...] as list\n",
    "            for subx_seq in subX.T:\n",
    "                counts = {}  # dict\n",
    "                for val in subx_seq.tolist():\n",
    "                    if val not in counts:\n",
    "                        counts[val] = 0\n",
    "                    counts[val] += 1\n",
    "                    #\"\"\"Problem 2. calcluate the P(X1=x11|Y=y), P(X1=x21|Y=y), hint: use dict() data type.\"\"\"\n",
    "                x_probs = {key: value / len(subX) for key, value in counts.items()}\n",
    "                y_cond_prob.append(x_probs) # the dimsension of y_cond_prob is (2,4) in Iris dataset.\n",
    "                \n",
    "            Py = len(subX) / len(X) # calculate P(Y=y)\n",
    "            self._probas.append((Py, y_cond_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e519ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ipynb.fs.full.module import StatisticClassifier\n",
    "from base.module import StatisticClassifier\n",
    "\n",
    "class _Tree:\n",
    "    \n",
    "    \"\"\"basic node structure for construction tree\"\"\"\n",
    "    \n",
    "    def __init__(self, label, feature=None):\n",
    "        \"\"\"Both `label` and `feature` are integers,\n",
    "           `feature` is the feature index for the next splitting,\n",
    "           `label` is the predict label at this node\n",
    "        \"\"\"\n",
    "        assert (isinstance(feature, int) and 0 <= feature) or feature is None\n",
    "        assert isinstance(label, (int, np.int32, np.int64)) and 0 <= label\n",
    "        self._feature = feature\n",
    "        self._label = label\n",
    "        self._children = {} #dict\n",
    "\n",
    "    @property\n",
    "    def label(self):\n",
    "        return self._label\n",
    "\n",
    "    @property\n",
    "    def feature(self):\n",
    "        return self._feature\n",
    "        \n",
    "    def __getitem__(self, condition):\n",
    "        return self._children.get(condition, self._label)\n",
    "\n",
    "    def __setitem__(self, condition, children):\n",
    "        assert not self.is_leaf(), 'current node is a leaf!'\n",
    "        assert isinstance(children, _Tree)\n",
    "        self._children[condition] = children\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self._feature is None\n",
    "\n",
    "\n",
    "def standard_entropy(seq):\n",
    "    assert len(seq.shape) == 1\n",
    "    uniqs, counts = np.unique(seq, return_counts=True)\n",
    "    probs = counts / seq.size\n",
    "    entropy = - sum (probs * np.log2(probs))     #\"\"\"Problem 1. entropy = -sum[P(y) * log(P(y))]\"\"\"\n",
    "    return entropy\n",
    "\n",
    "def conditional_entropy(x_seq, y_seq):\n",
    "    assert len(x_seq.shape) == len(y_seq.shape) == 1\n",
    "    assert x_seq.size == y_seq.size\n",
    "    entropy = 0.0\n",
    "    x_uniqs, x_counts = np.unique(x_seq, return_counts=True)\n",
    "    for x_label, x_count in zip(x_uniqs, x_counts):\n",
    "        x_prob = x_count / x_seq.size\n",
    "        y_controled_by_x = y_seq[x_seq == x_label] \n",
    "                         #\"Problem 2. conditional entropy = - Px * sum[P(y|x) * logP(y|x)]\" completed\n",
    "            #broadcast together with shapes (2,) (36,) \n",
    "        entropy += x_prob * (sum (((np.unique(y_controled_by_x, return_counts=True)[1])/len(y_controled_by_x))*\n",
    "                                 (np.log2((np.unique(y_controled_by_x, return_counts=True)[1])/len(y_controled_by_x)))))          \n",
    "    return - entropy\n",
    "        \n",
    "\n",
    "class DecisionTreeClassifier(StatisticClassifier):\n",
    "    def __init__(self):\n",
    "        StatisticClassifier.__init__(self, \"DecisionTree\")\n",
    "        self._tree = None\n",
    "\n",
    "    def forward(self, sample):\n",
    "        tree = self._tree\n",
    "        while not tree.is_leaf():\n",
    "            tree = tree[sample[tree.feature]]\n",
    "        return tree.label\n",
    "\n",
    "    def _construct_tree(self, X, Y, used_fids):\n",
    "        #We generate the decision tree recursively. Specifically, there\n",
    "        #are three steps as following:\n",
    "        #Step-1: check whether we stop the recursion.\n",
    "        #Step-2: calculate information gain of each variable to pick up\n",
    "        #        the best variable to split data.\n",
    "        #Step-3: recursively call this function to generate children\n",
    "        #        by using the best splitting variable.\n",
    "\n",
    "        # Step-1: check whether we need to generate children\n",
    "        most_freq_y = np.bincount(Y.astype(np.int32)).argmax() # y = 0,1,2,..\n",
    "        entire_entropy = standard_entropy(Y)\n",
    "        stop_cond_1 = (len (np.unique(Y))==1)\n",
    "                     #\"Problem 3. assign a bool value to identify: whether there is only a single y in Y.\"\n",
    "        stop_cond_2 = (np.unique(X, axis=0).shape[0] == 1)\n",
    "                     #\"Problem 4. assign a bool value to identify: whether we use out all features.\"\n",
    "        if stop_cond_1 or stop_cond_2:\n",
    "            return _Tree(most_freq_y) \n",
    "        \n",
    "        # Step-2: find out the best splitting feature\n",
    "        max_info_gain, best_fid, best_seq = -float(\"inf\"), 0, None\n",
    "        for fid, x_seq in enumerate(X.T):\n",
    "            if fid in used_fids:\n",
    "                continue\n",
    "            x_info_gain = entire_entropy - conditional_entropy(x_seq, Y)\n",
    "            if x_info_gain > max_info_gain:\n",
    "                max_info_gain, best_fid, best_seq = x_info_gain, fid, x_seq\n",
    "\n",
    "        # Step-3: recursively generate children of the current tree\n",
    "        root = _Tree(most_freq_y, best_fid)\n",
    "        used_fids = used_fids | {best_fid}\n",
    "        for uniq_val in np.unique(best_seq):\n",
    "            uniq_idx = best_seq == uniq_val\n",
    "            subX, subY = X[uniq_idx], Y[uniq_idx]\n",
    "                     #\"Problem 5. recursively call `self._construct_tree` to generate children.\"\n",
    "            root[uniq_val] = self._construct_tree(subX, subY, used_fids)\n",
    "        return root\n",
    "\n",
    "    def _fit(self, X, Y):\n",
    "        self._tree = self._construct_tree(X, Y, set())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5499001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: LogReg\n",
      "--------------------------------------------------------------------------------\n",
      "Train Accuracy=0.9714 | F1=0.9737 | AUC=0.9967\n",
      "Test Accuracy=0.9667 | F1=0.9600 | AUC=0.9955\n",
      "\n",
      "Model: MLP\n",
      "--------------------------------------------------------------------------------\n",
      "Train Accuracy=0.9857 | F1=0.9867 | AUC=1.0000\n",
      "Test Accuracy=0.9667 | F1=0.9600 | AUC=0.9910\n",
      "\n",
      "Model: NaiveBayesian\n",
      "--------------------------------------------------------------------------------\n",
      "Train Accuracy=0.9286 | F1=0.9296 | AUC=0.9308\n",
      "Test Accuracy=0.9333 | F1=0.9231 | AUC=0.9321\n",
      "\n",
      "Model: DecisionTree\n",
      "--------------------------------------------------------------------------------\n",
      "Train Accuracy=0.9857 | F1=0.9867 | AUC=0.9848\n",
      "Test Accuracy=0.9000 | F1=0.8800 | AUC=0.8937\n"
     ]
    }
   ],
   "source": [
    "# from ipynb.fs.full.utils import prepare_dataset, scoring\n",
    "from base.utils import prepare_dataset, scoring\n",
    "\n",
    "def pipeline(model, train, test):\n",
    "    name = model.name\n",
    "    trainX, trainY = train[:, :-1], train[:, -1]\n",
    "    print(\"\")\n",
    "    print(\"Model: %s\\n\" % name + \"-\" * 80)\n",
    "    model.fit(trainX, trainY)\n",
    "    \n",
    "    acc, f1, auc = scoring(trainY, model.predict(trainX))\n",
    "    print(\"Train Accuracy=%.4f | F1=%.4f | AUC=%.4f\" % (acc, f1, auc))\n",
    "\n",
    "    acc, f1, auc = scoring(test[:, -1].astype(np.int32), model.predict(test[:, :-1]))\n",
    "    print(\"Test Accuracy=%.4f | F1=%.4f | AUC=%.4f\" % (acc, f1, auc))\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    train, test, labels = prepare_dataset(\"./iris.csv\", do_normalize=True)\n",
    "    for architect in [LogisticRegression, MLPBinaryClassifier]:\n",
    "        pipeline(architect(), train, test)\n",
    "    train, test, labels = prepare_dataset(\"./iris.csv\", do_discretize=True)\n",
    "    for architect in [BayesianClassifier, DecisionTreeClassifier]:\n",
    "        pipeline(architect(), train, test)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
